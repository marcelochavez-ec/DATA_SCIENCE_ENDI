





# Permite ajustar la anchura de la parte útil de la libreta (reduce los márgenes)
from IPython.core.display import display, HTML
display(HTML("<style>.container{ width:80% !important; }</style>"))
import warnings
warnings.filterwarnings("ignore")








# Importar librerías
import wooldridge as wd
import pandas as pd
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LinearRegression

# Cargar el dataset "wage1" de wooldridge
df = wd.data("wage1")

# Definir variables predictoras (X) y variable respuesta (Y)
X_reg = df[["educ", "exper", "tenure"]]
Y_reg = df["wage"]

# Definir KFold correctamente
kfold = KFold(n_splits=10, shuffle=True, random_state=7)

# Modelo de regresión lineal
model = LinearRegression()

# Definir la métrica de evaluación
scoring = "neg_mean_squared_error"

# Realizar validación cruzada
results = cross_val_score(model, X_reg, Y_reg, cv=kfold, scoring=scoring)

# Imprimir el MSE promedio
print(f"MSE: {results.mean()}")


# Importar librerías
import wooldridge as wd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LinearRegression

# Cargar el dataset "wage1" de wooldridge
df = wd.data("wage1")

# Definir variables predictoras (X) y variable respuesta (Y)
X_reg = df[["educ"]]  # Usamos solo "educ" para la gráfica
Y_reg = df["wage"]

# Crear el modelo de regresión lineal
model = LinearRegression()
model.fit(X_reg, Y_reg)

# Hacer predicciones
X_range = np.linspace(X_reg.min(), X_reg.max(), 100).reshape(-1, 1)  # Rango de valores para "educ"
Y_pred = model.predict(X_range)

# Graficar datos reales
plt.scatter(X_reg, Y_reg, color="blue", alpha=0.5, label="Datos reales")

# Graficar la línea de regresión
plt.plot(X_range, Y_pred, color="red", linewidth=2, label="Regresión lineal")

# Personalizar la gráfica
plt.xlabel("Años de educación")
plt.ylabel("Salario (wage)")
plt.title("Regresión Lineal: Educación vs Salario")
plt.legend()
plt.grid(True)
plt.show()


# Importar librerías necesarias
from sklearn.linear_model import Ridge

# Definir el modelo Ridge con un valor de alpha (hiperparámetro de regularización)
ridge_model = Ridge(alpha=1.0)

# Ajustar el modelo Ridge con los datos
ridge_model.fit(X_reg, Y_reg)

# Hacer predicciones con Ridge
Y_ridge_pred = ridge_model.predict(X_range)

# Graficar comparación entre Regresión Lineal y Ridge
plt.scatter(X_reg, Y_reg, color="blue", alpha=0.5, label="Datos reales")
plt.plot(X_range, Y_pred, color="red", linewidth=2, label="Regresión Lineal")  # Línea roja: Regresión Lineal
plt.plot(X_range, Y_ridge_pred, color="green", linewidth=2, linestyle="--", label="Ridge Regression")  # Línea discontinua verde: Ridge

# Personalizar la gráfica
plt.xlabel("Años de educación")
plt.ylabel("Salario (wage)")
plt.title("Regresión Lineal vs Ridge Regression")
plt.legend()
plt.grid(True)
plt.show()








#importing libraries
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score








# Clasification problem
import pandas as pd
filename = 'data/pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
df_cla = pd.read_csv(filename, names=names)
array = df_cla.values
X_cla = array[:,0:8]
Y_cla = array[:,8]


df_cla





import pandas as pd
filename = 'data/housing.csv'
names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
df_reg = pd.read_csv(filename, delim_whitespace=True, names=names) 
array = df_reg.values
X_reg = array[:,0:13]
Y_reg = array[:,13]























# Logistic Regression Classification
???
# En la nueva versión cuando se pone random_state, entonces se tiene que poner shuffle=True
kfold = KFold(n_splits=10, random_state=7, shuffle=True)
???











# LDA Classification
???

kfold = KFold(n_splits=10, random_state=7, shuffle=True)
???

















# KNN Classification
???

kfold = KFold(n_splits=10, random_state=7, shuffle=True)
???











# Gaussian Naive Bayes Classification
???

kfold = KFold(n_splits=10, random_state=7, shuffle=True)
???











# CART Classification
???

kfold = KFold(n_splits=10, random_state=7, shuffle=True
model = DecisionTreeClassifier()
## También se podría especificar parámetros como indica la clase en sklearn
## En la nueva versión ya no puede ser max_features='sqrt', sino que tiene que ser max_features='sqrt'
#model = DecisionTreeClassifier(criterion='entropy', max_features='sqrt')
???











# SVM Classification
???

kfold = KFold(n_splits=10, random_state=7, shuffle=True)
???


























# Linear Regression
???

kfold= KFold(n_splits=10, random_state=7, shuffle=True)
???











# Ridge Regression
???

kfold= KFold(n_splits=10, random_state=7, shuffle=True)
???











# LASSO Regression
???

kfold= KFold(n_splits=10, random_state=7, shuffle=True)
???











# ElasticNet Regression
???

kfold= KFold(n_splits=10, random_state=7, shuffle=True)
???

















# k-NN Regressionore
???

kfold= KFold(n_splits=10, random_state=7, shuffle=True)
???











# CART Regression
???

kfold= KFold(n_splits=10, random_state=7, shuffle=True)
# en la nueva versión criterion='absolute_error' ya no puede ser criterion='mae'
model = DecisionTreeRegressor()
## También se podría especificar parámetros como indica la clase en sklearn
## En la nueva versión criterion='absolute_error' ya no puede ser criterion='mae'
#model = DecisionTreeRegressor(criterion='absolute_error')
???











# CART Regression
???

kfold= KFold(n_splits=10, random_state=7, shuffle=True)
???






